# ğŸ§  Quantum Support Vector Machine (QSVM) Explanation

## Overview
Quantum Support Vector Machine (QSVM) is a quantum-enhanced version of the classical Support Vector Machine that leverages quantum computing to perform classification tasks with potential speedup in high-dimensional feature spaces.

---

## ğŸ”¬ Classical vs Quantum SVM

### **Classical SVM:**
- **Kernel Trick**: Maps data to high-dimensional space using kernel functions
- **Complexity**: O(nÂ²) to O(nÂ³) for training, where n = number of samples
- **Limitation**: Kernel computation becomes expensive for large datasets

### **Quantum SVM:**
- **Quantum Feature Map**: Maps classical data to quantum state space
- **Quantum Kernel**: Computes inner products in quantum Hilbert space
- **Advantage**: Can handle exponentially large feature spaces efficiently

---

## ğŸ¯ Core Concepts

### **1. Quantum Feature Map**
Transforms classical data points into quantum states using parameterized quantum circuits.

```python
def quantum_feature_map(x, feature_dimension):
    """
    Maps classical data x to quantum state |Ï†(x)âŸ©
    
    Args:
        x: Classical data point (array)
        feature_dimension: Number of qubits for encoding
    
    Returns:
        QuantumCircuit: Circuit representing |Ï†(x)âŸ©
    """
    qr = QuantumRegister(feature_dimension, 'q')
    circuit = QuantumCircuit(qr)
    
    # Encode classical data into quantum state
    for i, feature in enumerate(x):
        # Apply rotation gates based on feature values
        circuit.rx(feature, qr[i])
        circuit.rz(feature, qr[i])
    
    return circuit
```

### **2. Quantum Kernel Computation**
Computes the inner product between quantum feature maps.

```python
def quantum_kernel(x1, x2, feature_dimension):
    """
    Computes quantum kernel K(x1, x2) = |âŸ¨Ï†(x1)|Ï†(x2)âŸ©|Â²
    
    Args:
        x1, x2: Classical data points
        feature_dimension: Number of qubits
    
    Returns:
        float: Kernel value between 0 and 1
    """
    # Create quantum feature maps
    circuit1 = quantum_feature_map(x1, feature_dimension)
    circuit2 = quantum_feature_map(x2, feature_dimension)
    
    # Create kernel estimation circuit
    kernel_circuit = create_kernel_circuit(circuit1, circuit2)
    
    # Measure overlap
    sampler = Sampler()
    job = sampler.run(kernel_circuit, shots=1000)
    result = job.result()
    
    # Extract kernel value from measurement
    kernel_value = extract_kernel_from_measurement(result)
    
    return kernel_value
```

### **3. Kernel Circuit Construction**
```python
def create_kernel_circuit(circuit1, circuit2):
    """
    Creates circuit to estimate |âŸ¨Ï†(x1)|Ï†(x2)âŸ©|Â²
    """
    n_qubits = circuit1.num_qubits
    
    # Create quantum registers
    qr = QuantumRegister(n_qubits, 'q')
    cr = ClassicalRegister(n_qubits, 'c')
    kernel_circuit = QuantumCircuit(qr, cr)
    
    # Apply first feature map
    kernel_circuit.compose(circuit1, inplace=True)
    
    # Apply inverse of second feature map
    circuit2_inv = circuit2.inverse()
    kernel_circuit.compose(circuit2_inv, inplace=True)
    
    # Measure all qubits
    kernel_circuit.measure_all()
    
    return kernel_circuit
```

---

## ğŸš€ QSVM Algorithm Implementation

### **1. Training Phase**
```python
def train_qsvm(X_train, y_train, feature_dimension):
    """
    Trains Quantum SVM model
    
    Args:
        X_train: Training features (n_samples, n_features)
        y_train: Training labels (-1 or 1)
        feature_dimension: Number of qubits for encoding
    
    Returns:
        dict: Trained model parameters
    """
    n_samples = len(X_train)
    
    # Compute quantum kernel matrix
    kernel_matrix = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
            kernel_matrix[i, j] = quantum_kernel(
                X_train[i], X_train[j], feature_dimension
            )
    
    # Solve dual SVM optimization problem
    # min Î±: (1/2)Î±^T K Î± - Î±^T y
    # subject to: 0 â‰¤ Î±_i â‰¤ C, Î£ Î±_i y_i = 0
    
    # Use classical optimization solver
    from scipy.optimize import minimize
    
    def objective(alpha):
        return 0.5 * alpha.T @ kernel_matrix @ alpha - alpha.T @ y_train
    
    def constraint(alpha):
        return alpha.T @ y_train  # Î£ Î±_i y_i = 0
    
    # Solve optimization
    result = minimize(
        objective, 
        np.zeros(n_samples),
        constraints={'type': 'eq', 'fun': constraint},
        bounds=[(0, C)] * n_samples
    )
    
    # Extract support vectors
    support_vector_indices = np.where(result.x > 1e-5)[0]
    support_vectors = X_train[support_vector_indices]
    support_vector_alphas = result.x[support_vector_indices]
    support_vector_labels = y_train[support_vector_indices]
    
    return {
        'support_vectors': support_vectors,
        'alphas': support_vector_alphas,
        'labels': support_vector_labels,
        'feature_dimension': feature_dimension
    }
```

### **2. Prediction Phase**
```python
def predict_qsvm(model, X_test):
    """
    Makes predictions using trained QSVM model
    
    Args:
        model: Trained QSVM model
        X_test: Test features
    
    Returns:
        array: Predicted labels (-1 or 1)
    """
    predictions = []
    
    for x_test in X_test:
        # Compute decision function
        decision_value = 0
        
        for i, (sv, alpha, label) in enumerate(zip(
            model['support_vectors'], 
            model['alphas'], 
            model['labels']
        )):
            # Compute kernel between test point and support vector
            kernel_val = quantum_kernel(
                x_test, sv, model['feature_dimension']
            )
            decision_value += alpha * label * kernel_val
        
        # Apply sign function
        prediction = 1 if decision_value > 0 else -1
        predictions.append(prediction)
    
    return np.array(predictions)
```

---

## ğŸ“Š Expected Outputs and Results

### **1. Training Results**
```
QSVM Training Results:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dataset: 2D Classification Problem
Training Samples: 100
Feature Dimension: 4 qubits
Support Vectors Found: 15
Training Accuracy: 94.0%

Kernel Matrix Shape: (100, 100)
Average Kernel Computation Time: 0.15s
```

### **2. Classification Visualization**
```
Prediction Results:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test Samples: 50
Correct Predictions: 47
Accuracy: 94.0%

Decision Boundary:
- Linear separation in quantum feature space
- Non-linear separation in original space
- Support vectors define boundary
```

### **3. Quantum Measurements**
```
Kernel Computation Example:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Data Points: x1 = [0.5, 0.3], x2 = [0.7, 0.1]
Feature Map: |Ï†(x1)âŸ© = RX(0.5)âŠ—RX(0.3)|0âŸ©
             |Ï†(x2)âŸ© = RX(0.7)âŠ—RX(0.1)|0âŸ©

Quantum Circuit:
|0âŸ© â”€â”€ RX(0.5) â”€â”€ RXâ»Â¹(0.7) â”€â”€ M
|0âŸ© â”€â”€ RX(0.3) â”€â”€ RXâ»Â¹(0.1) â”€â”€ M

Measurement Results:
|00âŸ©: 850 counts (85.0%)
|01âŸ©: 95 counts (9.5%)
|10âŸ©: 45 counts (4.5%)
|11âŸ©: 10 counts (1.0%)

Kernel Value: K(x1, x2) = 0.85
```

---

## ğŸ¯ Advanced Features

### **1. Custom Feature Maps**
```python
def custom_feature_map(x, feature_dimension):
    """
    Custom quantum feature map with entanglement
    """
    qr = QuantumRegister(feature_dimension, 'q')
    circuit = QuantumCircuit(qr)
    
    # Encode features with rotations
    for i, feature in enumerate(x):
        circuit.rx(feature, qr[i])
        circuit.rz(feature, qr[i])
    
    # Add entanglement layers
    for i in range(feature_dimension - 1):
        circuit.cx(qr[i], qr[i + 1])
        circuit.rz(x[i] * x[i + 1], qr[i + 1])
    
    return circuit
```

### **2. Multi-Class Classification**
```python
def multiclass_qsvm(X_train, y_train, n_classes):
    """
    Extends QSVM to multi-class classification
    using One-vs-One approach
    """
    models = {}
    
    # Train binary classifiers for each pair
    for i in range(n_classes):
        for j in range(i + 1, n_classes):
            # Create binary dataset
            mask = (y_train == i) | (y_train == j)
            X_binary = X_train[mask]
            y_binary = np.where(y_train[mask] == i, 1, -1)
            
            # Train binary QSVM
            model = train_qsvm(X_binary, y_binary, feature_dimension)
            models[(i, j)] = model
    
    return models
```

### **3. Quantum Kernel Optimization**
```python
def optimize_quantum_kernel(X_train, y_train, feature_dimension):
    """
    Optimizes quantum feature map parameters
    """
    def kernel_objective(params):
        # Update feature map with new parameters
        updated_kernel_matrix = compute_kernel_matrix(X_train, params)
        
        # Train SVM and compute cross-validation score
        cv_score = cross_validate_svm(updated_kernel_matrix, y_train)
        return -cv_score  # Minimize negative score
    
    # Optimize parameters
    from scipy.optimize import minimize
    result = minimize(kernel_objective, initial_params)
    
    return result.x
```

---

## ğŸ”§ Implementation Template

### **Complete QSVM Pipeline**
```python
def qsvm_pipeline(X_train, y_train, X_test, y_test, feature_dimension=4):
    """
    Complete QSVM classification pipeline
    """
    print("ğŸš€ Starting QSVM Training...")
    
    # 1. Train QSVM model
    model = train_qsvm(X_train, y_train, feature_dimension)
    
    print(f"âœ… Training Complete!")
    print(f"   Support Vectors: {len(model['support_vectors'])}")
    
    # 2. Make predictions
    predictions = predict_qsvm(model, X_test)
    
    # 3. Evaluate performance
    accuracy = np.mean(predictions == y_test)
    
    print(f"ğŸ“Š Test Accuracy: {accuracy:.3f}")
    
    # 4. Visualize results
    visualize_qsvm_results(X_train, y_train, X_test, predictions, model)
    
    return model, predictions, accuracy
```

### **Kernel Matrix Computation**
```python
def compute_kernel_matrix(X, feature_dimension):
    """
    Computes full quantum kernel matrix
    """
    n_samples = len(X)
    kernel_matrix = np.zeros((n_samples, n_samples))
    
    print("ğŸ”„ Computing Quantum Kernel Matrix...")
    
    for i in range(n_samples):
        for j in range(i, n_samples):
            kernel_val = quantum_kernel(X[i], X[j], feature_dimension)
            kernel_matrix[i, j] = kernel_val
            kernel_matrix[j, i] = kernel_val  # Symmetric
    
    print("âœ… Kernel Matrix Complete!")
    return kernel_matrix
```

---

## ğŸ“ˆ Performance Analysis

### **Quantum vs Classical Comparison**

| Metric | Classical SVM | Quantum SVM | Improvement |
|--------|---------------|-------------|-------------|
| **Training Time** | O(nÂ²) | O(nÂ²) | Similar |
| **Kernel Computation** | O(dÂ²) | O(log d) | Exponential |
| **Feature Space** | Limited by memory | Exponential | Exponential |
| **Expressiveness** | Kernel dependent | Quantum advantage | Higher |

### **Scalability Analysis**
```
Feature Dimension Scaling:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Classical: Memory grows as O(2^d)
Quantum: Memory grows as O(d)

For d = 20 features:
- Classical: 2^20 = 1,048,576 dimensions
- Quantum: 20 qubits needed
```

---

## ğŸ“ Educational Value

### **Key Learning Objectives:**
1. **Quantum Feature Maps**: Understanding quantum encoding of classical data
2. **Quantum Kernels**: Computing inner products in quantum Hilbert space
3. **Hybrid Quantum-Classical**: Combining quantum and classical optimization
4. **Quantum Advantage**: When and why quantum methods are beneficial

### **Interactive Demonstrations:**
- **Real-time kernel computation**: Watch quantum circuits being built
- **Feature map visualization**: See classical data mapped to quantum states
- **Decision boundary plots**: Visualize classification results
- **Performance comparisons**: Compare with classical SVM

---

## ğŸš€ Future Enhancements

1. **Error Mitigation**: Implement error correction for noisy quantum hardware
2. **Variational Quantum Kernels**: Learn optimal feature maps
3. **Quantum Neural Networks**: Integrate with quantum neural architectures
4. **Real Hardware Deployment**: Scale to actual quantum computers

---

*QSVM demonstrates how quantum computing can enhance classical machine learning algorithms by providing access to exponentially large feature spaces through quantum feature maps and kernel methods.* 